{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load modules.py\n",
    "#!/usr/bin/env python\n",
    "# @Project      : DeepLearningProject2\n",
    "# @Author       : Xiaoyu LIN\n",
    "# @File         : modules.py\n",
    "# @Description  :\n",
    "import math\n",
    "import torch\n",
    "from torch import FloatTensor, LongTensor, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    \"\"\"\n",
    "    Fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, hidden_units):\n",
    "        \"\"\"\n",
    "        :param input_units: number of the input tensor\n",
    "        :param hidden_units: number of hidden units in the layer\n",
    "        \n",
    "        \"\"\"\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    " #       self.input = FloatTensor(input_size)\n",
    " #       self.output = FloatTensor(hidden_units)\n",
    "\n",
    "        self.weights = FloatTensor(hidden_units, input_units).normal_(0, math.sqrt(2/input_units))\n",
    "        self.biases = FloatTensor(hidden_units).zero_()\n",
    "\n",
    "        self.grad_wrt_weights = [FloatTensor(hidden_units, input_units).zero_()]\n",
    "        self.grad_wrt_biases = [FloatTensor(hidden_units).zero_()]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self.output = input.matmul(self.weights.t()) + self.biases\n",
    "#        raise NotImplementedError\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_wrt_output):\n",
    "        \"\"\"\n",
    "\n",
    "        :param grad_wrt_output:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.grad_wrt_weights = grad_wrt_output.t().matmul(self.input)\n",
    "        self.grad_wrt_biases =  grad_wrt_output.sum(0)\n",
    "        self.grad_wrt_input = grad_wrt_output.mutmul(self.weights)\n",
    "#        raise NotImplementedError\n",
    "        return self.grad_wrt_input\n",
    "\n",
    "    def gradient_descent(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs the weights and biases updates\n",
    "        :param step_size: step size of the updates\n",
    "        \"\"\"\n",
    "        # updating weights and biases\n",
    "        self.weights -= learning_rate * self.grad_wrt_weights\n",
    "        self.biases -= step_size * self.grad_wrt_biases\n",
    "        \n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        parameters = {\"weights\": self.weights, \"biases\": self.biases}\n",
    "        gradients = {\"grad_wrt_weights\": self.grad_wrt_weights, \"grad_wrt_biases\": self.grad_wrt_biases,\"grad_wrt_input\": self.grad_wrt_input}\n",
    "\n",
    "        return parameters, gradients\n",
    "\n",
    "class Tanh():\n",
    "    \"\"\"\n",
    "    Tanh function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = self.input.tanh_()\n",
    "#        raise NotImplementedError\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_wrt_output):\n",
    "#        raise NotImplementedError\n",
    "        self.grad_wrt_input = grad_wrt_output * (1 - self.output * self.output)\n",
    "    \n",
    "        return self.grad_wrt_input\n",
    "#    def param(self):\n",
    "#        return []\n",
    "\n",
    "\n",
    "class ReLU(object):\n",
    "    \"\"\"\n",
    "    ReLu function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = self.input.relu_()\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self,grad_wrt_output):\n",
    "        derivative = self.input\n",
    "        derivative[derivative > 0.0] = 1.0\n",
    "        derivative[derivative <= 0.0] = 0.0\n",
    "        self.grad_wrt_input = grad_wrt_output * derivative\n",
    "        \n",
    "        return self.grad_wrt_input\n",
    "#    def param(self):\n",
    "#        return []\n",
    "\n",
    "\n",
    "class Sequential(object):\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class LossMSE(object):\n",
    "    \"\"\"\n",
    "    compute the MSE loss.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_loss(predictions, targets):\n",
    "        self.predictions = predictions\n",
    "        self.targets = targets\n",
    "        self.samples_num = predictions.size(0)\n",
    "        targets.reshape([-1,1])\n",
    "        predictions.reshape([-1,1])\n",
    "        # make sure shapes are the same\n",
    "        assert predictions.shape == targets.shape\n",
    "        \n",
    "        return ((predictions-targets)**2).mean(0, True).item()/self.samples_num\n",
    "    \n",
    "    def compute_grad():\n",
    "        self.grad_wrt_pred = ((predictions-targets)*2)/self.samples_num\n",
    "        \n",
    "        return self.grad_wrt_pred\n",
    "#    def forward(self, input):\n",
    "#        raise NotImplementedError\n",
    "\n",
    "#    def backward(self, gradwrtoutput):\n",
    "\n",
    "#        return ((predictions-targets)*2)/self.samples_num\n",
    "#    def param(self):\n",
    "#        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}